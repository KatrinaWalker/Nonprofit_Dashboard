Directory structure:

establish a working directory path on your computer (or in the AWS instance).
then a folder called "dataset" and inside of it, three folders: "ez", "pf" and "990"

The whole think will look like this:

\wrokingdirectory\dataset\990
\wrokingdirectory\dataset\pf
\wrokingdirectory\dataset\ez

(remember, case sensitivity)

-on \workingdirectory\ (call it as you like) will be saved the .csv files produced from the R script that later will be dumped on the SQL schema;
-on \990\ has to be placed all 990 csv files and only them;
-on \pf\ has to be placed all 990 csv files and only them;
-on \ez\ has to be placed all 990 csv files and only them;

BEFORE starting, en folder \ez\ you have to run on bash (terminal) the command stored on the folder "5. cleaning". It will change the "ein" column name to "EIN"

To run the whole thing
once the raw csv files are on its place:

- run the R script on "3. select_data" (there's only one script); IMPORTANT: you have to adjust the line that states the working directory. For example, now says
work.path <- "C:/ALFONSO/WORK/GSE/14D003/"
change it to
work.path <- "~/LAURA/GSE/14D003/" or however it woks on Mac (I don't knot)
Then source the code. It takes about 5 minutes.

Then go to SQL, and load the script on "4. schema"
The same thing, pay attention (at the end of the script) of the path stated where it has to be located the dumping csv files (the ones generated by R).
It also takes around 5 minutes (in local, on AWS... I have no idea, there's about 500 MB of data to be loaded).
You will see some warnings (truncated data and such) on the data dumping and some errors at the end, on the key creation.
But, anyway, you can check the tables happily loaded.


